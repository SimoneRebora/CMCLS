{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preparation\n",
        "\n",
        "**Before you start this notebook,** you need to:\n",
        "- Create a shortcut of the \"CMCLS\" folder in your Google Drive (so you can access the curated annotations)\n",
        "- Verify that the Notebook uses a GPU (in the menu, \"Runtime\" -> \"Change runtime type\" and select any available Hardware accelerator\n",
        "with a GPU)\n",
        "- Create an account at https://wandb.ai/site (\"Signup\" -> \"Signup with Google\", and select \"Academic/University\" account type; you can immediately downgrade to a Free account); Login and keep the window open"
      ],
      "metadata": {
        "id": "gM7NIKcyzNlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once everything is ready, you can start preparing the machine learning procedure.  \n",
        "**First step:** Install all libraries  \n",
        "*(Note that you will have to authorize connecting the Notebook to your Google Drive)*"
      ],
      "metadata": {
        "id": "OnQjItQ8p6Ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjWQ51XbQR3b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "from transformers import AutoConfig\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import EvalPrediction\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statistics import mean\n",
        "import numpy as np\n",
        "from datasets import DatasetDict, Dataset, Features, ClassLabel, Value\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second step:** Define functions"
      ],
      "metadata": {
        "id": "UsjLppUXp-fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence\"], max_length=256, truncation=True, padding=\"max_length\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    # calculate accuracy using sklearn's function\n",
        "    mf1 = f1_score(labels, preds, average='macro')\n",
        "    wf1 = f1_score(labels, preds, average='weighted')\n",
        "    return {\n",
        "        'mf1': mf1,\n",
        "        'wf1': wf1,\n",
        "    }\n",
        "\n",
        "def predict_text_class(input_text, labels, model, tokenizer):\n",
        "\n",
        "    input_ids = tokenizer(input_text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        output = model(**input_ids)\n",
        "\n",
        "    predicted_labels = output.logits.argmax(dim=1)\n",
        "\n",
        "    return labels[predicted_labels.item()]"
      ],
      "metadata": {
        "id": "kVOs7NfpeFr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third step:** Read the dataset and show it"
      ],
      "metadata": {
        "id": "cwo7VuXfqBJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the dataset\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/CMCLS/curation.xlsx\")\n",
        "\n",
        "# list labels\n",
        "labels = list(set(df['curation']))\n",
        "for label in labels:\n",
        "  print(f'curation = {label}, count = {df[\"curation\"].tolist().count(label)}')\n",
        "\n",
        "# show data\n",
        "df = df[['sentence', 'curation']]\n",
        "df = df.rename(columns={'curation': 'label'})\n",
        "df['sentence'] = df['sentence'].astype('string')\n",
        "df"
      ],
      "metadata": {
        "id": "HP5S4BHHQtnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training and testing"
      ],
      "metadata": {
        "id": "S2lPHNelfQXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Train base model\n",
        "\n",
        "Let's start by training a basic model, [*google-bert/bert-base-uncased*](https://huggingface.co/google-bert/bert-base-uncased)"
      ],
      "metadata": {
        "id": "xFavms5QtMO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the pretrained model\n",
        "checkpoint = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "# define all training arguments\n",
        "training_args = TrainingArguments(\"/content\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    overwrite_output_dir=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model='wf1',\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# split dataset into train and test\n",
        "# we select \"Group A\" as test\n",
        "train_set = df[98:]\n",
        "test = df[:98]\n",
        "\n",
        "# split train set into train and validation sets\n",
        "train, val, y_train, y_val = train_test_split(train_set, train_set['label'], test_size=0.1, random_state=42)\n",
        "\n",
        "# create datasets\n",
        "dataset_train = Dataset.from_pandas(train, features=Features({\"sentence\": Value(dtype='string'), \"label\": ClassLabel(names=labels)}), preserve_index=False)\n",
        "dataset = DatasetDict([(\"train\", dataset_train)])\n",
        "dataset_val = Dataset.from_pandas(val, features=Features(\n",
        "    {\"sentence\": Value(dtype='string'), \"label\": ClassLabel(names=labels)}), preserve_index=False)\n",
        "dataset['val'] = dataset_val\n",
        "\n",
        "# load model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3, ignore_mismatched_sizes=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# move the model to 'cuda' to leverage GPU during the finetuning\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# tokenize the train and evaluation set\n",
        "tokenized_train = dataset['train'].map(tokenize_function, batched=True)\n",
        "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "tokenized_val = dataset['val'].map(tokenize_function, batched=True)\n",
        "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
        "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "#Â send the model to the device (cuda)\n",
        "model.to(device)\n",
        "\n",
        "# define the trainer object\n",
        "trainer = Trainer(\n",
        "model=model,\n",
        "args=training_args,\n",
        "train_dataset=tokenized_train,\n",
        "eval_dataset=tokenized_val,\n",
        "compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "# switch the model back to cpu, otherwise (I don't know why) it doesn't do the prediction\n",
        "model.to('cpu')\n",
        "\n",
        "# make predictions\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for i in range(test.shape[0]):\n",
        "  sentence = test.iloc[i,0]\n",
        "  true_labels.append(test.iloc[i,1])\n",
        "  predicted_labels.append(predict_text_class(sentence, labels=labels, model=model, tokenizer=tokenizer))\n",
        "\n",
        "# print and save report\n",
        "report = classification_report(true_labels,predicted_labels,digits=3)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "IAdgF4EmlJ4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Finetune pretrained model\n",
        "\n",
        "Then we can finetune a model already trained for sentiment analysis:  \n",
        "[*cardiffnlp/twitter-roberta-base-sentiment-latest*](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)"
      ],
      "metadata": {
        "id": "N0bLxgJ0WXuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the pretrained model\n",
        "checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# define all training arguments\n",
        "training_args = TrainingArguments(\"/content\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=12,\n",
        "    per_device_eval_batch_size=12,\n",
        "    overwrite_output_dir=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model='wf1',\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# split dataset into train and test\n",
        "# we select \"Group A\" as test\n",
        "train_set = df[98:]\n",
        "test = df[:98]\n",
        "\n",
        "# split train set into train and validation sets\n",
        "train, val, y_train, y_val = train_test_split(train_set, train_set['label'], test_size=0.1, random_state=42)\n",
        "\n",
        "# create datasets\n",
        "dataset_train = Dataset.from_pandas(train, features=Features({\"sentence\": Value(dtype='string'), \"label\": ClassLabel(names=labels)}), preserve_index=False)\n",
        "dataset = DatasetDict([(\"train\", dataset_train)])\n",
        "dataset_val = Dataset.from_pandas(val, features=Features(\n",
        "    {\"sentence\": Value(dtype='string'), \"label\": ClassLabel(names=labels)}), preserve_index=False)\n",
        "dataset['val'] = dataset_val\n",
        "\n",
        "# load model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3, ignore_mismatched_sizes=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# move the model to 'cuda' to leverage GPU during the finetuning\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# tokenize the train and evaluation set\n",
        "tokenized_train = dataset['train'].map(tokenize_function, batched=True)\n",
        "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "tokenized_val = dataset['val'].map(tokenize_function, batched=True)\n",
        "tokenized_val = tokenized_val.rename_column(\"label\", \"labels\")\n",
        "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "#Â send the model to the device (cuda)\n",
        "model.to(device)\n",
        "\n",
        "# define the trainer object\n",
        "trainer = Trainer(\n",
        "model=model,\n",
        "args=training_args,\n",
        "train_dataset=tokenized_train,\n",
        "eval_dataset=tokenized_val,\n",
        "compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "# switch the model back to cpu, otherwise (I don't know why) it doesn't do the prediction\n",
        "model.to('cpu')\n",
        "\n",
        "# make predictions\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for i in range(test.shape[0]):\n",
        "  sentence = test.iloc[i,0]\n",
        "  true_labels.append(test.iloc[i,1])\n",
        "  predicted_labels.append(predict_text_class(sentence, labels=labels, model=model, tokenizer=tokenizer))\n",
        "\n",
        "# print and save report\n",
        "report = classification_report(true_labels,predicted_labels,digits=3)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "b7kJVPr9WN3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Apply the model\n",
        "\n",
        "Finally, we can try the model on a new sentence."
      ],
      "metadata": {
        "id": "1clMXA1M26eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I liked this book\"\n",
        "\n",
        "predict_text_class(sentence, labels=labels, model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "K9UPi9pI29PY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}