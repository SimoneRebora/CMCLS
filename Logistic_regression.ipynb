{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preparation"
      ],
      "metadata": {
        "id": "w8x2lO_Alpp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a **Python Notebook**. To make it work, you need to press \"play\" on the code cells.  \n",
        "\n",
        "*Remember to always run cells in the right order and never skip one!*  \n",
        "In case of doubt, you can always restart from the beginning."
      ],
      "metadata": {
        "id": "ZBblm8IezIs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you start, you need to \"clone\" the course GitHub repository into the Python notebook.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i63NOLB-nvJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SimoneRebora/CMCLS.git"
      ],
      "metadata": {
        "id": "JPYSWyA7ft7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can read the \"sample\" dataset, which contains sentences from the \"corpus\" dateset annotated for valence.  \n",
        "Annotations were created during the \"Distant Reading in R\" workshop with the help of ten annotators: https://github.com/ABC-DH/Distant_Reading_in_R/tree/main/Machine_Learning_Files  \n",
        "We can upload and take a look at the dataset."
      ],
      "metadata": {
        "id": "mNMT0XY4gqwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "my_df = pd.read_csv('CMCLS/materials/ML_sample_dataset.csv', index_col=0)\n",
        "my_df"
      ],
      "metadata": {
        "id": "1p0V9zUGh_xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Machine Learning\n"
      ],
      "metadata": {
        "id": "4m2oi4xv4rSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings for texts\n",
        "\n",
        "We'll be using SentiArt to create embeddings.  "
      ],
      "metadata": {
        "id": "wEbIBISKtp9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK data if you haven't already\n",
        "nltk.download('punkt')\n",
        "\n",
        "# read the SentiArt dictionary\n",
        "TC = 'CMCLS/materials/SentiArt.csv' # for English texts\n",
        "sa = pd.read_csv(TC)\n",
        "\n",
        "# tokenize sentences\n",
        "sents = my_df['text']\n",
        "tokens = [[t for t in word_tokenize(s) if t.isalpha()] for s in sents]\n",
        "\n",
        "# compute mean AAP (or mean fear etc.) per sentence\n",
        "sent_mean_ang_z = [];sent_mean_fear_z = [];sent_mean_disg_z = [];sent_mean_hap_z = [];sent_mean_sad_z = [];sent_mean_surp_z = []\n",
        "for t in tokens:\n",
        "    dt = sa.query('word in @t')\n",
        "    sent_mean_ang_z.append(dt.ang_z.mean())\n",
        "    sent_mean_fear_z.append(dt.fear_z.mean())\n",
        "    sent_mean_disg_z.append(dt.disg_z.mean())\n",
        "    sent_mean_hap_z.append(dt.hap_z.mean())\n",
        "    sent_mean_sad_z.append(dt.sad_z.mean())\n",
        "    sent_mean_surp_z.append(dt.surp_z.mean())\n",
        "\n",
        "# add results to table\n",
        "my_df['anger'] = sent_mean_ang_z\n",
        "my_df['fear'] = sent_mean_fear_z\n",
        "my_df['disgust'] = sent_mean_disg_z\n",
        "my_df['happiness'] = sent_mean_hap_z\n",
        "my_df['sadness'] = sent_mean_sad_z\n",
        "my_df['surprise'] = sent_mean_surp_z\n",
        "\n",
        "# remove NAs\n",
        "my_df.fillna(0, inplace=True)\n",
        "\n",
        "# visualize\n",
        "my_df = round(my_df,3)\n",
        "my_df"
      ],
      "metadata": {
        "id": "mRpMOnNCBC9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression\n",
        "\n",
        "Train a logistic regression model with SentiArt embeddings (to predict evaluation vs. other)"
      ],
      "metadata": {
        "id": "fG-ep_bfqYVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset in Train and Test"
      ],
      "metadata": {
        "id": "_7EnhjfRvMXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# create tables for train and test\n",
        "train, test = train_test_split(my_df, test_size=0.2, random_state = 123) # our main data split into train and test\n",
        "# the attribute test_size=0.2 splits the data into 80% and 20% ratio. train=80% and test=20%\n",
        "# the attribute random_state=123 randomizes the selection of sentences in a repeatable way\n",
        "\n",
        "# separate training embeddings from training labels\n",
        "train_X = train[['anger','fear','disgust','happiness','sadness','surprise']] # taking the training data features\n",
        "train_y = train.label # output of the training data\n",
        "\n",
        "# separate test embeddings from test labels\n",
        "test_X = test[['anger','fear','disgust','happiness','sadness','surprise']] # taking test data feature\n",
        "test_y = test.label # output value of the test data\n",
        "\n",
        "# print shapes of train/test embedding tables\n",
        "print(train_X.shape)\n",
        "print(test_X.shape)\n",
        "\n",
        "# show train material\n",
        "train_X"
      ],
      "metadata": {
        "id": "1qGJlTvFqdDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run logistic regression and see the results in the table"
      ],
      "metadata": {
        "id": "DEHkHtqCs3jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# fit a LR model to our train data\n",
        "model = LogisticRegression()\n",
        "model.fit(train_X, train_y)\n",
        "\n",
        "# make prediction with model on test data\n",
        "prediction = model.predict(test_X)\n",
        "\n",
        "# add results to test table and show\n",
        "test['logistic regression'] = prediction\n",
        "test"
      ],
      "metadata": {
        "id": "9nxjRgqsrhU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the efficiency scores"
      ],
      "metadata": {
        "id": "F-HFkH-Ms18a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_y, prediction))"
      ],
      "metadata": {
        "id": "14Uwz3YRs1XI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}