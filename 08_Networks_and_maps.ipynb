{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "This is a Python Notebook. To make it work, you need to press \"play\" on the code cells.  \n",
        "Remember to always run cells in the right order and never skip one!\n",
        "In case of doubt, you can always restart from the beginning.\n",
        "\n",
        "First, let's clone the GitHub repository."
      ],
      "metadata": {
        "id": "tM8cqdpdeDke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SimoneRebora/CMCLS.git"
      ],
      "metadata": {
        "id": "ZPJjXdRLFzqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we extract the named entities with [spaCy](https://spacy.io/)"
      ],
      "metadata": {
        "id": "u2__z6YneJ-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cetlOlPp31b"
      },
      "outputs": [],
      "source": [
        "# define text to be analyzed\n",
        "my_text = 'CMCLS/corpus/Doyle_Study_1887.txt'\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Install spacy and download a model if not already present\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "with open(my_text, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Process the text with Spacy\n",
        "doc = nlp(text)\n",
        "\n",
        "# extract entities per sentence\n",
        "entities_with_sentences = []\n",
        "sentences = list(doc.sents) # Convert sentences to a list to access by index\n",
        "\n",
        "for ent in doc.ents:\n",
        "    for i, sent in enumerate(sentences):\n",
        "        if ent.start >= sent.start and ent.end <= sent.end:\n",
        "            entities_with_sentences.append((ent.text, ent.label_, i)) # Store sentence index\n",
        "            break # Found the sentence, move to the next entity\n",
        "\n",
        "df = pd.DataFrame(entities_with_sentences)\n",
        "df.columns = ['Entity', 'Type', 'Sentence Index']\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Network\n",
        "\n",
        "First, we extract all \"PERSON\" entities.  \n",
        "Then we calculate the co-occurrences of persons in adjacent sentences (in the same or in the following one).  \n",
        "We finally save the results into \"nodes\" and \"edges\" tables and show them."
      ],
      "metadata": {
        "id": "IBptnZuEeh77"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9faabfb9"
      },
      "source": [
        "import itertools\n",
        "\n",
        "# filter person entities\n",
        "person_entities_df = df[df['Type'] == 'PERSON']\n",
        "# group entities per sentence\n",
        "grouped_persons = person_entities_df.groupby('Sentence Index')['Entity'].apply(lambda x: list(x.unique())).to_dict()\n",
        "\n",
        "# calculate co-occurrences\n",
        "# 1. Initialize an empty dictionary to store co-occurrence relationships and their weights\n",
        "co_occurrence_counts = {}\n",
        "\n",
        "# 2. Get a sorted list of unique sentence indices from the grouped_persons dictionary\n",
        "sorted_sentence_indices = sorted(grouped_persons.keys())\n",
        "\n",
        "# 3. Iterate through the sorted list of sentence indices\n",
        "for i, current_sentence_index in enumerate(sorted_sentence_indices):\n",
        "    # a. Retrieve the list of person entities for the current_sentence_index\n",
        "    persons_in_current_sentence = grouped_persons.get(current_sentence_index, [])\n",
        "\n",
        "    # b. Determine the next_sentence_index\n",
        "    next_sentence_index = current_sentence_index + 1\n",
        "\n",
        "    # c. Check if next_sentence_index exists and retrieve persons\n",
        "    persons_in_next_sentence = grouped_persons.get(next_sentence_index, [])\n",
        "\n",
        "    # d. Combine persons from current and next sentences into a single set\n",
        "    all_co_occurring_persons = set(persons_in_current_sentence + persons_in_next_sentence)\n",
        "\n",
        "    # e. If all_co_occurring_persons contains more than one person, iterate through all unique pairs\n",
        "    if len(all_co_occurring_persons) > 1:\n",
        "        for person1, person2 in itertools.combinations(sorted(list(all_co_occurring_persons)), 2):\n",
        "            # i. Ensure the pair is ordered alphabetically\n",
        "            ordered_pair = tuple(sorted((person1, person2)))\n",
        "\n",
        "            # ii. Increment the count for this ordered pair\n",
        "            co_occurrence_counts[ordered_pair] = co_occurrence_counts.get(ordered_pair, 0) + 1\n",
        "\n",
        "# save and show the table\n",
        "co_occurrence_df = pd.DataFrame([\n",
        "    {'source': pair[0], 'target': pair[1], 'weight': count}\n",
        "    for pair, count in co_occurrence_counts.items()\n",
        "])\n",
        "\n",
        "co_occurrence_df.to_csv('co_occurrence_df.csv', index=False)\n",
        "\n",
        "co_occurrence_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can (or, actually, should) curate the co-occurrences a little bit.  \n",
        "We can do it by downloading the .csv file to our computer, correcting it, and uploading it again.\n",
        "\n",
        "_______________________\n",
        "**Correction can be done with software like [LibreOffice](https://it.libreoffice.org/download/download/)**\n",
        "_______________________\n",
        "\n",
        "Then we can re-read the co-occurrences file and convert it into \"nodes\" and \"edges\" tables."
      ],
      "metadata": {
        "id": "DXXrQ_PJlNUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re-read the file\n",
        "co_occurrence_df = pd.read_csv('co_occurrence_df.csv')\n",
        "\n",
        "# Get all unique entities from 'source' and 'target' columns\n",
        "unique_entities = pd.concat([co_occurrence_df['source'], co_occurrence_df['target']]).unique()\n",
        "\n",
        "# Create a new DataFrame with 'ID' and 'label' columns\n",
        "nodes_df = pd.DataFrame({\n",
        "    'ID': unique_entities,\n",
        "    'label': unique_entities\n",
        "})\n",
        "\n",
        "# save nodes and edges\n",
        "nodes_df.to_csv('nodes_df.csv', index=False)\n",
        "co_occurrence_df.to_csv('edges_df.csv', index=False)\n",
        "\n",
        "# show them\n",
        "display(nodes_df)\n",
        "display(co_occurrence_df)"
      ],
      "metadata": {
        "id": "o1Srs-4Jl8mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can print the network (not very nice, though...)"
      ],
      "metadata": {
        "id": "hzVJCH60fHqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges from the co_occurrence_df\n",
        "for index, row in co_occurrence_df.iterrows():\n",
        "    G.add_edge(row['source'], row['target'], weight=row['weight'])\n",
        "\n",
        "# show the plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "pos = nx.spring_layout(G, k=0.8, iterations=20) # You can experiment with different layouts and parameters\n",
        "nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=2000)\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', width=1.0, alpha=0.7)\n",
        "nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
        "\n",
        "plt.title(\"Co-occurrence Network of PERSON Entities\")\n",
        "plt.axis('off') # Hide axes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2D03LNlQHe98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Mapping\n",
        "\n",
        "First, we extract the \"GPE\" or \"LOC\" entities.  \n",
        "And we geolocate them (using the [geopy](https://geopy.readthedocs.io/en/stable/) package)."
      ],
      "metadata": {
        "id": "DVuD59VWfjf1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abbf66d"
      },
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "location_entities_df = df[df['Type'].isin(['GPE', 'LOC'])]['Entity'].unique()\n",
        "location_entities_df = pd.DataFrame(location_entities_df, columns=['Location'])\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"colab_geocoder\", timeout=10) # Increased timeout to 10 seconds\n",
        "\n",
        "location_entities_df['Latitude'] = None\n",
        "location_entities_df['Longitude'] = None\n",
        "\n",
        "for index, row in location_entities_df.iterrows():\n",
        "    location_name = row['Location']\n",
        "    print(f\"Geocoding {location_name}...\")\n",
        "    try:\n",
        "        geodata = geolocator.geocode(location_name)\n",
        "        if geodata:\n",
        "            location_entities_df.loc[index, 'Latitude'] = geodata.latitude\n",
        "            location_entities_df.loc[index, 'Longitude'] = geodata.longitude\n",
        "    except Exception as e:\n",
        "        print(f\"Error geocoding {location_name}: {e}\") # Removed the \\n from inside the f-string\n",
        "    time.sleep(1) # Add a delay of 1 second between requests\n",
        "\n",
        "location_entities_df.to_csv(\"locations_df.csv\")\n",
        "\n",
        "location_entities_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can (or, actually, should) curate the geolocations a little bit.  \n",
        "We can do it by downloading the .csv file to our computer, correcting it, and uploading it again.\n",
        "\n",
        "_______________________\n",
        "**Correction can be done with software like [LibreOffice](https://it.libreoffice.org/download/download/)  \n",
        "Note that you can use [GoogleMaps](https://www.google.com/maps) to find coordinates**\n",
        "_______________________\n",
        "\n",
        "\n",
        "Finally, we can show the map (still, not very good)"
      ],
      "metadata": {
        "id": "tYltBpNHgHx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "\n",
        "# re-read the table\n",
        "location_entities_df = pd.read_csv(\"locations_df.csv\")\n",
        "\n",
        "# Create a base map centered around an approximate average of the locations\n",
        "# You might want to adjust the initial center or zoom level\n",
        "center_lat = location_entities_df['Latitude'].mean()\n",
        "center_lon = location_entities_df['Longitude'].mean()\n",
        "m = folium.Map(location=[center_lat, center_lon], zoom_start=2)\n",
        "\n",
        "# Add markers for each location\n",
        "for index, row in location_entities_df.iterrows():\n",
        "    if pd.notna(row['Latitude']) and pd.notna(row['Longitude']):\n",
        "        folium.Marker(\n",
        "            location=[row['Latitude'], row['Longitude']],\n",
        "            popup=row['Location']\n",
        "        ).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "display(m)"
      ],
      "metadata": {
        "id": "lCd_mOeVNKU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}